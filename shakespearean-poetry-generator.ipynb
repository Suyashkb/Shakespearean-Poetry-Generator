{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-23T14:54:09.268898Z","iopub.execute_input":"2023-08-23T14:54:09.269296Z","iopub.status.idle":"2023-08-23T14:54:09.278539Z","shell.execute_reply.started":"2023-08-23T14:54:09.269264Z","shell.execute_reply":"2023-08-23T14:54:09.276722Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf \nimport time \nimport os\npath_to_file = tf.keras.utils.get_file(\n    \"shakespeare.txt\",\n    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n)\nprint(\"Done\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:09.280373Z","iopub.execute_input":"2023-08-23T14:54:09.280641Z","iopub.status.idle":"2023-08-23T14:54:09.298659Z","shell.execute_reply.started":"2023-08-23T14:54:09.280618Z","shell.execute_reply":"2023-08-23T14:54:09.297739Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Imported the file and uploaded the data of shakespeare text ","metadata":{}},{"cell_type":"code","source":"text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\nprint(f\"Length of text: {len(text)} characters\")\nprint(text[:250])","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:09.307274Z","iopub.execute_input":"2023-08-23T14:54:09.307906Z","iopub.status.idle":"2023-08-23T14:54:09.314452Z","shell.execute_reply.started":"2023-08-23T14:54:09.307872Z","shell.execute_reply":"2023-08-23T14:54:09.313350Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Length of text: 1115394 characters\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Put the data into 'Text' and checked the first 250 words and printed the total number of words ","metadata":{}},{"cell_type":"code","source":"vocab = sorted(set(text))\nprint(f\"{len(vocab)} unique characters\")\nprint(vocab)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:09.316879Z","iopub.execute_input":"2023-08-23T14:54:09.317606Z","iopub.status.idle":"2023-08-23T14:54:09.345983Z","shell.execute_reply.started":"2023-08-23T14:54:09.317563Z","shell.execute_reply":"2023-08-23T14:54:09.344904Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"65 unique characters\n['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Sorted all the data and pulled out the unique characters ","metadata":{}},{"cell_type":"code","source":"example_texts = [\"abcdefg\", \"xyz\"]\n\nchars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\nchars\nids_from_chars = tf.keras.layers.StringLookup(\n    vocabulary=list(vocab), mask_token=None\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:09.347507Z","iopub.execute_input":"2023-08-23T14:54:09.347922Z","iopub.status.idle":"2023-08-23T14:54:09.370396Z","shell.execute_reply.started":"2023-08-23T14:54:09.347890Z","shell.execute_reply":"2023-08-23T14:54:09.369529Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"The text is vectorized here i.e. The letters are converted into vectors/tensors.","metadata":{}},{"cell_type":"markdown","source":"Created StringLookup layer from tensorflow using tf.keras.layers.StringLookup adn inputed in ids_from_chars ","metadata":{}},{"cell_type":"code","source":"ids = ids_from_chars(chars)\nids","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:09.373964Z","iopub.execute_input":"2023-08-23T14:54:09.374261Z","iopub.status.idle":"2023-08-23T14:54:09.385992Z","shell.execute_reply.started":"2023-08-23T14:54:09.374237Z","shell.execute_reply":"2023-08-23T14:54:09.385110Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"},"metadata":{}}]},{"cell_type":"markdown","source":"It converts tokens from character to numeric ids to feed into the model .\n","metadata":{}},{"cell_type":"code","source":"chars_from_ids = tf.keras.layers.StringLookup(\n    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:09.387395Z","iopub.execute_input":"2023-08-23T14:54:09.387868Z","iopub.status.idle":"2023-08-23T14:54:09.402092Z","shell.execute_reply.started":"2023-08-23T14:54:09.387835Z","shell.execute_reply":"2023-08-23T14:54:09.401226Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"This layer recovers the characters from the vectors of IDs, and returns them as a 'tf.RaggedTensor' of characters.since invert is true it helps in retrieving characters m  ","metadata":{}},{"cell_type":"code","source":"chars = chars_from_ids(ids)\nchars","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:09.403502Z","iopub.execute_input":"2023-08-23T14:54:09.403844Z","iopub.status.idle":"2023-08-23T14:54:09.411077Z","shell.execute_reply.started":"2023-08-23T14:54:09.403812Z","shell.execute_reply":"2023-08-23T14:54:09.410126Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"},"metadata":{}}]},{"cell_type":"markdown","source":"characters are joined back","metadata":{}},{"cell_type":"code","source":"tf.strings.reduce_join(chars, axis=-1).numpy()\ndef text_from_ids(ids):\n    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:09.412478Z","iopub.execute_input":"2023-08-23T14:54:09.413104Z","iopub.status.idle":"2023-08-23T14:54:09.434711Z","shell.execute_reply.started":"2023-08-23T14:54:09.413053Z","shell.execute_reply":"2023-08-23T14:54:09.433764Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"You can tf.strings.reduce_join to join the characters back into strings.","metadata":{}},{"cell_type":"markdown","source":"**Prediction task**\nCreating training examples and targets","metadata":{}},{"cell_type":"code","source":"\nall_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\nall_ids\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:09.437787Z","iopub.execute_input":"2023-08-23T14:54:09.438640Z","iopub.status.idle":"2023-08-23T14:54:09.947193Z","shell.execute_reply.started":"2023-08-23T14:54:09.438615Z","shell.execute_reply":"2023-08-23T14:54:09.946096Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"},"metadata":{}}]},{"cell_type":"markdown","source":" \nEach unicode is converted to a corresponding id using utf endcoding and converted to tensors(Tensors arae stored as all_ids)\nall_ids in next line displays the tensor/value in console of the program ","metadata":{}},{"cell_type":"code","source":"ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\nfor ids in ids_dataset.take(10):\n    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:09.948557Z","iopub.execute_input":"2023-08-23T14:54:09.948945Z","iopub.status.idle":"2023-08-23T14:54:09.980777Z","shell.execute_reply.started":"2023-08-23T14:54:09.948908Z","shell.execute_reply":"2023-08-23T14:54:09.979730Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"F\ni\nr\ns\nt\n \nC\ni\nt\ni\n","output_type":"stream"}]},{"cell_type":"markdown","source":"ids_dataset creates a dataset consisting of all the ","metadata":{}},{"cell_type":"code","source":"seq_length = 100\nexamples_per_epoch = len(text) // (seq_length + 1)\n\nsequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n\nfor seq in sequences.take(1):\n    print(chars_from_ids(seq))","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:09.982378Z","iopub.execute_input":"2023-08-23T14:54:09.982755Z","iopub.status.idle":"2023-08-23T14:54:10.002698Z","shell.execute_reply.started":"2023-08-23T14:54:09.982719Z","shell.execute_reply":"2023-08-23T14:54:10.001734Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"tf.Tensor(\n[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n b'o' b'u' b' '], shape=(101,), dtype=string)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Batch method lets you easily convert these individual characters to sequences of the desired size.","metadata":{}},{"cell_type":"code","source":"for seq in sequences.take(5):\n    print(text_from_ids(seq).numpy())","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:10.005648Z","iopub.execute_input":"2023-08-23T14:54:10.005945Z","iopub.status.idle":"2023-08-23T14:54:10.028232Z","shell.execute_reply.started":"2023-08-23T14:54:10.005919Z","shell.execute_reply":"2023-08-23T14:54:10.027235Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\nb'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\nb\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\nb\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\nb'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n","output_type":"stream"}]},{"cell_type":"code","source":"def split_input_target(sequence):\n    input_text = sequence[:-1]\n    target_text = sequence[1:]\n    return input_text, target_text\n\nsplit_input_target(list(\"Tensorflow\"))\ndataset = sequences.map(split_input_target)\nfor input_example, target_example in dataset.take(1):\n    print(\"Input :\", text_from_ids(input_example).numpy())\n    print(\"Target:\", text_from_ids(target_example).numpy())","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:10.029626Z","iopub.execute_input":"2023-08-23T14:54:10.029941Z","iopub.status.idle":"2023-08-23T14:54:10.076206Z","shell.execute_reply.started":"2023-08-23T14:54:10.029909Z","shell.execute_reply":"2023-08-23T14:54:10.075131Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\nTarget: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Dataset created as (input,label)by adjusting a pointer one space to the right and training the model to predict the next possible character(tensor)","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = (\n    dataset.shuffle(BUFFER_SIZE)\n    .batch(BATCH_SIZE, drop_remainder=True)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:10.077676Z","iopub.execute_input":"2023-08-23T14:54:10.078000Z","iopub.status.idle":"2023-08-23T14:54:10.091173Z","shell.execute_reply.started":"2023-08-23T14:54:10.077967Z","shell.execute_reply":"2023-08-23T14:54:10.089756Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"},"metadata":{}}]},{"cell_type":"markdown","source":"The data is already spilt using tf.data earlier the batch size is getting defined and teh data is shuffled.\n","metadata":{}},{"cell_type":"markdown","source":"**Model Building**","metadata":{}},{"cell_type":"code","source":"# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = 1024","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:10.093923Z","iopub.execute_input":"2023-08-23T14:54:10.094346Z","iopub.status.idle":"2023-08-23T14:54:10.100856Z","shell.execute_reply.started":"2023-08-23T14:54:10.094308Z","shell.execute_reply":"2023-08-23T14:54:10.099730Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"class MyModel(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, rnn_units):\n        super().__init__(self)\n        # TODO - Create an embedding layer\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        # TODO - Create a GRU layer\n        self.gru = tf.keras.layers.GRU(\n            rnn_units, return_sequences=True, return_state=True\n        )\n        # TODO - Finally connect it with a dense layer\n        self.dense = tf.keras.layers.Dense(vocab_size)\n\n    def call(self, inputs, states=None, return_state=False, training=False):\n        x = self.embedding(inputs, training=training)\n        # since we are training a text generation model,\n        # we use the previous state, in training. If there is no state,\n        # then we initialize the state\n        if states is None:\n            states = self.gru.get_initial_state(x)\n        x, states = self.gru(x, initial_state=states, training=training)\n        x = self.dense(x, training=training)\n\n        if return_state:\n            return x, states\n        else:\n            return x","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:10.102429Z","iopub.execute_input":"2023-08-23T14:54:10.103097Z","iopub.status.idle":"2023-08-23T14:54:10.114722Z","shell.execute_reply.started":"2023-08-23T14:54:10.103033Z","shell.execute_reply":"2023-08-23T14:54:10.113119Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"* tf.keras.layers.Embedding: The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;\n\n* tf.keras.layers.GRU: A type of RNN with size units=rnn_units (You can also use an LSTM layer here.);\n\n* tf.keras.layers.Dense: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.\n\nThe layers are defined in MyModel class under tf.keras.Model \nembedding and GRU layers used and connected with the dense layer \nself embedding done in the lower function \n\n","metadata":{}},{"cell_type":"code","source":"model = MyModel(\n    # Be sure the vocabulary size matches the `StringLookup` layers.\n    vocab_size=len(ids_from_chars.get_vocabulary()),\n    embedding_dim=embedding_dim,\n    rnn_units=rnn_units,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:10.116028Z","iopub.execute_input":"2023-08-23T14:54:10.116447Z","iopub.status.idle":"2023-08-23T14:54:10.134782Z","shell.execute_reply.started":"2023-08-23T14:54:10.116415Z","shell.execute_reply":"2023-08-23T14:54:10.133901Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"Mymodel provided to a variable ","metadata":{}},{"cell_type":"code","source":"for input_example_batch, target_example_batch in dataset.take(1):\n    example_batch_predictions = model(input_example_batch)\n    print(\n        example_batch_predictions.shape,\n        \"# (batch_size, sequence_length, vocab_size)\",\n    )\n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:10.139756Z","iopub.execute_input":"2023-08-23T14:54:10.140017Z","iopub.status.idle":"2023-08-23T14:54:13.288356Z","shell.execute_reply.started":"2023-08-23T14:54:10.139994Z","shell.execute_reply":"2023-08-23T14:54:13.280565Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"(64, 100, 66) # (batch_size, sequence_length, vocab_size)\nModel: \"my_model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_1 (Embedding)     multiple                  16896     \n                                                                 \n gru_1 (GRU)                 multiple                  3938304   \n                                                                 \n dense_1 (Dense)             multiple                  67650     \n                                                                 \n=================================================================\nTotal params: 4,022,850\nTrainable params: 4,022,850\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"sampled_indices = tf.random.categorical(\n    example_batch_predictions[0], num_samples=1\n)\nsampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\nsampled_indices","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:13.289593Z","iopub.execute_input":"2023-08-23T14:54:13.289922Z","iopub.status.idle":"2023-08-23T14:54:13.301379Z","shell.execute_reply.started":"2023-08-23T14:54:13.289892Z","shell.execute_reply":"2023-08-23T14:54:13.300207Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"array([57, 35, 38, 54, 64, 23, 60, 12, 16, 27, 23, 12, 62, 19, 37, 54, 27,\n       59, 39, 31, 54, 20,  6, 33, 55, 54, 55,  8, 34, 33, 44, 27, 41,  3,\n       31,  0, 43,  8, 21, 39,  9, 55, 58, 61, 24, 14, 11, 27, 38,  0, 20,\n        2, 60,  6, 50, 56, 42, 44, 55, 26,  5, 26, 65, 41, 30, 28, 13, 23,\n       24, 16, 17, 45, 16, 62, 37, 46, 53, 50,  3,  0, 51, 65, 41, 49, 49,\n       36, 32, 50, 28, 38,  7, 44, 13, 38, 52, 23, 26, 61, 28,  3])"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\nprint()\nprint(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:13.303023Z","iopub.execute_input":"2023-08-23T14:54:13.303703Z","iopub.status.idle":"2023-08-23T14:54:13.320019Z","shell.execute_reply.started":"2023-08-23T14:54:13.303667Z","shell.execute_reply":"2023-08-23T14:54:13.318700Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Input:\n b'loathsome world,\\nThan these poor compounds that thou mayst not sell.\\nI sell thee poison; thou hast s'\n\nNext Char Predictions:\n b\"rVYoyJu;CNJ;wFXoNtZRoG'Tpop-UTeNb!R[UNK]d-HZ.psvKA:NY[UNK]G u'kqcepM&MzbQO?JKCDfCwXgnk![UNK]lzbjjWSkOY,e?YmJMvO!\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Training the model**","metadata":{}},{"cell_type":"code","source":"loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\nexample_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\nprint(\n    \"Prediction shape: \",\n    example_batch_predictions.shape,\n    \" # (batch_size, sequence_length, vocab_size)\",\n)\nprint(\"Mean loss:        \", example_batch_mean_loss)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:13.321446Z","iopub.execute_input":"2023-08-23T14:54:13.322090Z","iopub.status.idle":"2023-08-23T14:54:13.349778Z","shell.execute_reply.started":"2023-08-23T14:54:13.322031Z","shell.execute_reply":"2023-08-23T14:54:13.345108Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\nMean loss:         tf.Tensor(4.188236, shape=(), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Created a loss variable and used it on the last layer (shape);during initializatioan of model training the logits are supposed to be of close magnitudes to make sure the model is not overconfident while creating the early responses and the weights and biases initially not satisfactory enough .this ensures proper changes being registed while reducing the lost function.","metadata":{}},{"cell_type":"code","source":"tf.exp(example_batch_mean_loss).numpy()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:13.351470Z","iopub.execute_input":"2023-08-23T14:54:13.351808Z","iopub.status.idle":"2023-08-23T14:54:13.360873Z","shell.execute_reply.started":"2023-08-23T14:54:13.351776Z","shell.execute_reply":"2023-08-23T14:54:13.359979Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"65.90644"},"metadata":{}}]},{"cell_type":"markdown","source":"Checked Exponential mean to be approximately equal to the vocabulary size. Higher loss represents that model is sure about its wrong answers and the training is badly initialised.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer=\"adam\", loss=loss)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:13.362268Z","iopub.execute_input":"2023-08-23T14:54:13.362832Z","iopub.status.idle":"2023-08-23T14:54:13.377325Z","shell.execute_reply.started":"2023-08-23T14:54:13.362800Z","shell.execute_reply":"2023-08-23T14:54:13.376140Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"Model is compiled using Adam optimiser and the loss is registered all along wih default arguments ","metadata":{}},{"cell_type":"markdown","source":"**Configuration of Checkpoints and Execution of training**","metadata":{}},{"cell_type":"code","source":"# Directory where the checkpoints will be saved\ncheckpoint_dir = \"./training_checkpoints\"\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix, save_weights_only=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:13.381259Z","iopub.execute_input":"2023-08-23T14:54:13.381992Z","iopub.status.idle":"2023-08-23T14:54:13.386685Z","shell.execute_reply.started":"2023-08-23T14:54:13.381954Z","shell.execute_reply":"2023-08-23T14:54:13.385898Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"Checkpoionts saved in a directory and ","metadata":{}},{"cell_type":"code","source":"EPOCHS = 10\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:54:13.387836Z","iopub.execute_input":"2023-08-23T14:54:13.388358Z","iopub.status.idle":"2023-08-23T14:55:43.876307Z","shell.execute_reply.started":"2023-08-23T14:54:13.388325Z","shell.execute_reply":"2023-08-23T14:55:43.875344Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Epoch 1/10\n172/172 [==============================] - 12s 42ms/step - loss: 2.7320\nEpoch 2/10\n172/172 [==============================] - 9s 39ms/step - loss: 1.9977\nEpoch 3/10\n172/172 [==============================] - 9s 38ms/step - loss: 1.7154\nEpoch 4/10\n172/172 [==============================] - 9s 39ms/step - loss: 1.5508\nEpoch 5/10\n172/172 [==============================] - 9s 38ms/step - loss: 1.4511\nEpoch 6/10\n172/172 [==============================] - 9s 39ms/step - loss: 1.3832\nEpoch 7/10\n172/172 [==============================] - 9s 39ms/step - loss: 1.3308\nEpoch 8/10\n172/172 [==============================] - 9s 38ms/step - loss: 1.2857\nEpoch 9/10\n172/172 [==============================] - 9s 39ms/step - loss: 1.2447\nEpoch 10/10\n172/172 [==============================] - 9s 38ms/step - loss: 1.2045\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Total number of epochs 10 to makesure the amount of time is monitored and not too much ","metadata":{}},{"cell_type":"markdown","source":"**Generation of text**","metadata":{}},{"cell_type":"markdown","source":"--------------------------------","metadata":{}},{"cell_type":"code","source":"class OneStep(tf.keras.Model):\n    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n        super().__init__()\n        self.temperature = temperature\n        self.model = model\n        self.chars_from_ids = chars_from_ids\n        self.ids_from_chars = ids_from_chars\n\n        # Create a mask to prevent \"[UNK]\" from being generated.\n        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n        sparse_mask = tf.SparseTensor(\n            # Put a -inf at each bad index.\n            values=[-float(\"inf\")] * len(skip_ids),\n            indices=skip_ids,\n            # Match the shape to the vocabulary\n            dense_shape=[len(ids_from_chars.get_vocabulary())],\n        )\n        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n\n    @tf.function\n    def generate_one_step(self, inputs, states=None):\n        # Convert strings to token IDs.\n        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n        input_ids = self.ids_from_chars(input_chars).to_tensor()\n\n        # Run the model.\n        # predicted_logits.shape is [batch, char, next_char_logits]\n        predicted_logits, states = self.model(\n            inputs=input_ids, states=states, return_state=True\n        )\n        # Only use the last prediction.\n        predicted_logits = predicted_logits[:, -1, :]\n        predicted_logits = predicted_logits / self.temperature\n        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n        predicted_logits = predicted_logits + self.prediction_mask\n\n        # Sample the output logits to generate token IDs.\n        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n\n        # Convert from token ids to characters\n        predicted_chars = self.chars_from_ids(predicted_ids)\n\n        # Return the characters and model state.\n        return predicted_chars, states\n    \none_step_model = OneStep(model, chars_from_ids, ids_from_chars)\nstart = time.time()\nstates = None\nnext_char = tf.constant([\"ROMEO:\"])\nresult = [next_char]\n\nfor n in range(1000):\n    next_char, states = one_step_model.generate_one_step(\n        next_char, states=states\n    )\n    result.append(next_char)\n\nresult = tf.strings.join(result)\nend = time.time()\nprint(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\nprint(\"\\nRun time:\", end - start)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:55:43.878302Z","iopub.execute_input":"2023-08-23T14:55:43.878653Z","iopub.status.idle":"2023-08-23T14:55:47.122744Z","shell.execute_reply.started":"2023-08-23T14:55:43.878617Z","shell.execute_reply":"2023-08-23T14:55:47.121733Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"ROMEO: Pryay a knavour weep.\n\nThird Gent. OVARD:\nBut I did scarvel to hear these vile?\nCome, then thou art thy boots: let me do.\n\nLADY ANNE:\nThen such gone ease mey in but as ever ever:\nlooks there with a was to do at the fillage.\n\nKING EDWARD IV:\nHeaven buy most cold was Marcius to her\nSome other, then.\n\nCitiser:\nNo more, be cousin;\nWhich of these cormest first.\n\nAUTOLYCUS:\nHe capets our conscretes: but not\narrive the battle pardon. This last, you have putting him:\nA stetion will restore by mind.\nCome, I will not made at the found to sit our fel:\nThe shepherdess of the fair presence: imposited me,\nIn deseral through can great world could not you know\nOver'd on earth, my lord; concrule our chequed\nThat this the rube's blessing creature of the proport?\n\nDUKE OF YORK:\nKing Ricemp! a sacrining of Exeter,\nThat now in peace is fair dead stemb'd King Henry's maid,\nAnd do not move her.\n\nHASTINGS:\nI'd the counce of this costainly service,\nI dare a pupure to himself as ever\nOs friendly give you o'er! \n\n________________________________________________________________________________\n\nRun time: 3.20888352394104\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}